from memory import UnsafePointer

# ANCHOR: softmax_gpu_kernel
from gpu import thread_idx, block_idx, block_dim, barrier
from gpu.host import DeviceContext, HostBuffer, DeviceBuffer
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb
from math import exp
from utils.numerics import max_finite, min_finite

# =============================================================================
# Batched Softmax GPU ì»¤ë„ êµ¬í˜„
# =============================================================================
#
# Batched Softmaxë€ ë¬´ì—‡ì¸ê°€?
# - 2D ì…ë ¥ í…ì„œì˜ ê° í–‰ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ softmaxë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜
# - ì˜ˆì‹œ: [[2.0, 1.0], [3.0, 0.1]] â†’ [[0.731, 0.269], [0.952, 0.048]]
# - ì£¼ìš©ë„: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—¬ëŸ¬ ìƒ˜í”Œì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” AI ëª¨ë¸
#
# Row-wise vs Column-wise Processing
# - Row-wise: ê° í–‰(ë°°ì¹˜ ìƒ˜í”Œ)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ softmax ì ìš©
# - Column-wise: ê° ì—´(íŠ¹ì„± ì°¨ì›)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ softmax ì ìš©
# - í˜„ì¬ êµ¬í˜„: Row-wise (ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš© íŒ¨í„´)

# 2D Batched Softmax ì„¤ì •
alias BATCH_SIZE = 8  # ë°°ì¹˜ í¬ê¸° (í–‰ ê°œìˆ˜)
alias FEATURE_SIZE = 512  # íŠ¹ì„± í¬ê¸° (ì—´ ê°œìˆ˜)
alias TPB = 128
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias layout = Layout.row_major(BATCH_SIZE, FEATURE_SIZE)

# =============================================================================
# Buffer Reduction ì „ëµì„ ìœ„í•œ ìƒˆë¡œìš´ GPU ì»¤ë„ë“¤ (2D Batched ë²„ì „)
# =============================================================================
#
# ë¬¸ì œ ì •ì˜: 2D ì…ë ¥ (BATCH_SIZE, FEATURE_SIZE)ì—ì„œ ê° í–‰ë³„ ë…ë¦½ ì²˜ë¦¬
# - ê¸°ì¡´: 1D ë²¡í„° (SIZE=512)ì— ëŒ€í•œ Buffer Reduction
# - ëª©í‘œ: 2D í…ì„œ (2, 256)ì—ì„œ ê° í–‰ë³„ë¡œ softmax ì ìš©
#
# 2D Batched Softmax ìµœì í™” ì „ëµ
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ ì²˜ë¦¬ ë°©ì‹       â”‚ ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ    â”‚ ì¥ì               â”‚ ë‹¨ì               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Row-wise       â”‚ ì—°ì†ëœ ë©”ëª¨ë¦¬     â”‚ ìºì‹œ ì§€ì—­ì„± ìš°ìˆ˜    â”‚ í–‰ë³„ ë™ê¸°í™” í•„ìš”   â”‚
# â”‚ (í˜„ì¬ êµ¬í˜„)     â”‚ ì ‘ê·¼ íŒ¨í„´         â”‚ ë©”ëª¨ë¦¬ íš¨ìœ¨ì       â”‚                  â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Column-wise    â”‚ ìŠ¤íŠ¸ë¼ì´ë“œ ì ‘ê·¼    â”‚ ë³‘ë ¬ì„± ê·¹ëŒ€í™”      â”‚ ìºì‹œ ë¯¸ìŠ¤ ì¦ê°€     â”‚
# â”‚                â”‚ íŒ¨í„´             â”‚                  â”‚ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì      â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ë‹¨ê³„: reduce_block_kernel_2d - ê° í–‰ë³„ ë¸”ë¡ ë‹¨ìœ„ ë¦¬ë•ì…˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# í•µì‹¬ ì•„ì´ë””ì–´: ê° í–‰ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜, í–‰ ë‚´ì—ì„œëŠ” ê¸°ì¡´ Buffer Reduction ì ìš©
#
# ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ (BATCH_SIZE=4, FEATURE_SIZE=128 ì˜ˆì‹œ):
# - Row 0: input[0, 0:128]   -> local_max_0, local_sum_0
# - Row 1: input[1, 0:128]   -> local_max_1, local_sum_1
# - Row 2: input[2, 0:128]   -> local_max_2, local_sum_2
# - Row 3: input[3, 0:128]   -> local_max_3, local_sum_3
#
# ê° í–‰ì€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì™„ë²½í•œ ë³‘ë ¬ì„± ë‹¬ì„±
# í–‰ ë‚´ì—ì„œëŠ” FEATURE_SIZE > TPBì¸ ê²½ìš° Buffer Reduction ì ìš©

# =============================================================================
# 2D Batched Softmaxë¥¼ ìœ„í•œ ìƒˆë¡œìš´ GPU ì»¤ë„ë“¤
# =============================================================================


fn reduce_block_kernel_2d[
    layout: Layout,
    batch_size: Int,
    feature_size: Int,
    dtype: DType = DType.float32,
](
    block_maxes: UnsafePointer[Scalar[dtype]],  # [batch_size * blocks_per_row]
    block_sums: UnsafePointer[Scalar[dtype]],  # [batch_size * blocks_per_row]
    input: LayoutTensor[dtype, layout, MutableAnyOrigin],
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2D Batched: 1ë‹¨ê³„ - ê° í–‰ë³„ ë¸”ë¡ ë‹¨ìœ„ ë¦¬ë•ì…˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬ ì•„ì´ë””ì–´: ê° í–‰ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜, í–‰ ë‚´ì—ì„œëŠ” Buffer Reduction ì ìš©
    #
    # ë©”ëª¨ë¦¬ êµ¬ì¡° (BATCH_SIZE=4, FEATURE_SIZE=128, TPB=128 ì˜ˆì‹œ):
    # - Row 0: input[0, 0:128] â†’ block_maxes[0], block_sums[0]
    # - Row 1: input[1, 0:128] â†’ block_maxes[1], block_sums[1]
    # - Row 2: input[2, 0:128] â†’ block_maxes[2], block_sums[2]
    # - Row 3: input[3, 0:128] â†’ block_maxes[3], block_sums[3]
    #
    # FEATURE_SIZE > TPBì¸ ê²½ìš°:
    # - ê° í–‰ì„ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
    # - blocks_per_row = (FEATURE_SIZE + TPB - 1) // TPB

    local_i = thread_idx.x
    block_id = block_idx.x

    # 2D ì¸ë±ì‹±: ì–´ë–¤ í–‰(batch)ê³¼ ì–´ë–¤ ë¸”ë¡(feature chunk)ì¸ì§€ ê³„ì‚°
    blocks_per_row = (feature_size + TPB - 1) // TPB
    batch_idx = block_id // blocks_per_row  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜ ì¸ë±ìŠ¤
    feature_block_idx = block_id % blocks_per_row  # í˜„ì¬ í–‰ ë‚´ ë¸”ë¡ ì¸ë±ìŠ¤

    # ê²½ê³„ ì²´í¬: ìœ íš¨í•œ ë°°ì¹˜ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì²˜ë¦¬
    if batch_idx >= batch_size:
        return

    # í˜„ì¬ ìŠ¤ë ˆë“œê°€ ë‹´ë‹¹í•  ì „ì—­ ì¸ë±ìŠ¤ ê³„ì‚°
    global_feature_idx = feature_block_idx * TPB + local_i

    # ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹ (ê¸°ì¡´ 1Dì™€ ë™ì¼í•œ íŒ¨í„´)
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-1ë‹¨ê³„: ê° í–‰ ë‚´ì—ì„œ ë¸”ë¡ë³„ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var thread_max: Scalar[dtype] = min_finite[dtype]()
    if global_feature_idx < feature_size:
        # 2D ì¸ë±ì‹±: input[batch_idx, global_feature_idx]
        thread_max = rebind[Scalar[dtype]](input[batch_idx, global_feature_idx])

    shared_max[local_i] = thread_max
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ìµœëŒ“ê°’ ì°¾ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2

    var block_max = shared_max[0]  # í˜„ì¬ ë¸”ë¡ì˜ ìµœëŒ“ê°’

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-2ë‹¨ê³„: ì§€ìˆ˜ ê³„ì‚° ë° ë¸”ë¡ë³„ í•©ê³„ êµ¬í•˜ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var exp_val: Scalar[dtype] = 0.0
    if global_feature_idx < feature_size:
        exp_val = rebind[Scalar[dtype]](
            exp(input[batch_idx, global_feature_idx] - block_max)
        )

    shared_sum[local_i] = exp_val
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ì§€ìˆ˜ í•©ê³„ êµ¬í•˜ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    var block_sum = shared_sum[0]  # í˜„ì¬ ë¸”ë¡ì˜ ì§€ìˆ˜ í•©ê³„

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-3ë‹¨ê³„: ë¸”ë¡ë³„ ê²°ê³¼ë¥¼ ì¤‘ê°„ ë²„í¼ì— ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if local_i == 0:  # ë¸”ë¡ì˜ ëŒ€í‘œ ìŠ¤ë ˆë“œë§Œ ì €ì¥
        block_maxes[block_id] = rebind[Scalar[dtype]](block_max)
        block_sums[block_id] = rebind[Scalar[dtype]](block_sum)


fn reduce_interim_kernel_2d[
    dtype: DType = DType.float32,
](
    final_vals: UnsafePointer[
        Scalar[dtype]
    ],  # [batch_size * 2] (ê° í–‰ì˜ [global_max, global_sum])
    block_maxes: UnsafePointer[Scalar[dtype]],
    block_sums: UnsafePointer[Scalar[dtype]],
    batch_size: Int,
    blocks_per_row: Int,
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2D Batched: 2ë‹¨ê³„ - ê° í–‰ë³„ ê¸€ë¡œë²Œ í†µê³„ ê³„ì‚°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬: ê° í–‰ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ê¸€ë¡œë²Œ maxì™€ sum ê³„ì‚°
    # - final_vals êµ¬ì¡°: [row0_max, row0_sum, row1_max, row1_sum, ...]

    local_i = thread_idx.x
    batch_idx = block_idx.x  # ê° ë¸”ë¡ì´ í•˜ë‚˜ì˜ ë°°ì¹˜(í–‰)ë¥¼ ë‹´ë‹¹

    # ê²½ê³„ ì²´í¬
    if batch_idx >= batch_size:
        return

    # ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-1ë‹¨ê³„: í˜„ì¬ í–‰ì˜ ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var thread_max: Scalar[dtype] = min_finite[dtype]()
    if local_i < blocks_per_row:
        # í˜„ì¬ í–‰ì˜ block_idx ê³„ì‚°
        row_block_idx = batch_idx * blocks_per_row + local_i
        thread_max = block_maxes[row_block_idx]

    shared_max[local_i] = thread_max
    barrier()

    # íŠ¸ë¦¬ ë¦¬ë•ì…˜ìœ¼ë¡œ í–‰ë³„ ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2

    var global_max = shared_max[0]  # í˜„ì¬ í–‰ì˜ ê¸€ë¡œë²Œ ìµœëŒ“ê°’

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-2ë‹¨ê³„: í˜„ì¬ í–‰ì˜ ê¸€ë¡œë²Œ í•©ê³„ ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var adjusted_sum: Scalar[dtype] = 0.0
    if local_i < blocks_per_row:
        row_block_idx = batch_idx * blocks_per_row + local_i
        adjusted_sum = block_sums[row_block_idx] * rebind[Scalar[dtype]](
            exp(block_maxes[row_block_idx] - global_max)
        )

    shared_sum[local_i] = adjusted_sum
    barrier()

    # íŠ¸ë¦¬ ë¦¬ë•ì…˜ìœ¼ë¡œ í–‰ë³„ ê¸€ë¡œë²Œ í•©ê³„ êµ¬í•˜ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    global_sum = shared_sum[0]  # í˜„ì¬ í–‰ì˜ ê¸€ë¡œë²Œ í•©ê³„

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-3ë‹¨ê³„: í–‰ë³„ ê²°ê³¼ë¥¼ ìµœì¢… ë²„í¼ì— ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if local_i == 0:
        final_vals[batch_idx * 2] = rebind[Scalar[dtype]](
            global_max
        )  # [batch_idx][0]
        final_vals[batch_idx * 2 + 1] = rebind[Scalar[dtype]](
            global_sum
        )  # [batch_idx][1]


fn normalize_kernel_2d[
    layout: Layout,
    batch_size: Int,
    feature_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[dtype, layout, MutableAnyOrigin],
    input: LayoutTensor[dtype, layout, MutableAnyOrigin],
    final_vals: UnsafePointer[Scalar[dtype]],  # [batch_size * 2]
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2D Batched: 3ë‹¨ê³„ - ê° í–‰ë³„ ìµœì¢… ì •ê·œí™”
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬: ê° í–‰ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ìµœì¢… softmax ì •ê·œí™” ìˆ˜í–‰
    # output[batch_idx, feature_idx] = exp(input[batch_idx, feature_idx] - global_max) / global_sum

    local_i = thread_idx.x
    block_id = block_idx.x

    # 2D ì¸ë±ì‹±: ì–´ë–¤ í–‰(batch)ê³¼ ì–´ë–¤ ë¸”ë¡(feature chunk)ì¸ì§€ ê³„ì‚°
    blocks_per_row = (feature_size + TPB - 1) // TPB
    batch_idx = block_id // blocks_per_row
    feature_block_idx = block_id % blocks_per_row

    # ê²½ê³„ ì²´í¬
    if batch_idx >= batch_size:
        return

    # í˜„ì¬ ìŠ¤ë ˆë“œê°€ ë‹´ë‹¹í•  ì „ì—­ íŠ¹ì„± ì¸ë±ìŠ¤
    global_feature_idx = feature_block_idx * TPB + local_i

    # ê²½ê³„ ì²´í¬: ìœ íš¨í•œ íŠ¹ì„± ë²”ìœ„ ë‚´ì—ì„œë§Œ ì²˜ë¦¬
    if global_feature_idx >= feature_size:
        return

    # í˜„ì¬ í–‰ì˜ ê¸€ë¡œë²Œ í†µê³„ ê°€ì ¸ì˜¤ê¸°
    global_max = final_vals[batch_idx * 2]  # í˜„ì¬ í–‰ì˜ global_max
    global_sum = final_vals[batch_idx * 2 + 1]  # í˜„ì¬ í–‰ì˜ global_sum

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 3ë‹¨ê³„: ìµœì¢… softmax ì •ê·œí™”
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ê³µì‹: output[i,j] = exp(input[i,j] - global_max_i) / global_sum_i
    # - ê° í–‰(i)ë³„ë¡œ ë…ë¦½ì ì¸ global_max_i, global_sum_i ì‚¬ìš©
    # - ì™„ë²½í•œ ë³‘ë ¬ ì²˜ë¦¬: ëª¨ë“  ì›ì†Œê°€ ë™ì‹œì— ê³„ì‚°ë¨
    output[batch_idx, global_feature_idx] = rebind[Scalar[dtype]](
        exp(input[batch_idx, global_feature_idx] - global_max) / global_sum
    )


# =============================================================================
# Buffer Reduction ì „ëµì„ ìœ„í•œ ìƒˆë¡œìš´ GPU ì»¤ë„ë“¤
# =============================================================================


fn reduce_block_kernel[
    layout: Layout,
    input_size: Int,
    dtype: DType = DType.float32,
](
    block_maxes: UnsafePointer[Scalar[dtype]],
    block_sums: UnsafePointer[Scalar[dtype]],
    input: LayoutTensor[mut=False, dtype, layout],
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1ë‹¨ê³„: Block-wise Reduction Kernel
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # ëª©ì : ëŒ€ê·œëª¨ ì…ë ¥ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê° ë¸”ë¡ì˜ local í†µê³„ ê³„ì‚°
    #
    # í•µì‹¬ ì „ëµ:
    # 1. ë°ì´í„° ë¶„í• : input[SIZE] â†’ chunks[GRID_SIZE][TPB]
    # 2. ë³‘ë ¬ ì²˜ë¦¬: ê° ë¸”ë¡ì´ ë…ë¦½ì ìœ¼ë¡œ local_max, local_sum ê³„ì‚°
    # 3. ê²°ê³¼ ì €ì¥: ì¤‘ê°„ ë²„í¼ì— ë¸”ë¡ë³„ ê²°ê³¼ ì €ì¥
    #
    # ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ìµœì í™”:
    # - Sequential Access: ê° ìŠ¤ë ˆë“œê°€ ì—°ì†ëœ ë©”ëª¨ë¦¬ ìœ„ì¹˜ ì ‘ê·¼
    # - Cache Locality: ë¸”ë¡ ë‚´ ìŠ¤ë ˆë“œë“¤ì´ ì¸ì ‘ ë°ì´í„° ì²˜ë¦¬
    # - Bandwidth Efficiency: coalesced memory accessë¡œ ìµœëŒ€ ì²˜ë¦¬ëŸ‰
    #
    # ì˜ˆì‹œ (SIZE=512, TPB=128, GRID_SIZE=4):
    # ```
    # Block 0: Thread 0~127 â†’ input[0:128]   â†’ local_max_0, local_sum_0
    # Block 1: Thread 0~127 â†’ input[128:256] â†’ local_max_1, local_sum_1
    # Block 2: Thread 0~127 â†’ input[256:384] â†’ local_max_2, local_sum_2
    # Block 3: Thread 0~127 â†’ input[384:512] â†’ local_max_3, local_sum_3
    # ```
    #
    # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê³ ë ¤ì‚¬í•­:
    # - ê° ë¸”ë¡ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ local_max ê³„ì‚°í•˜ì—¬ ì§€ìˆ˜ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    # - local_sum = Î£ exp(x_i - local_max) í˜•íƒœë¡œ ì•ˆì „í•œ ì§€ìˆ˜ ê³„ì‚°
    # - 2ë‹¨ê³„ì—ì„œ global_max ê¸°ì¤€ìœ¼ë¡œ ì¬ì¡°ì •í•˜ì—¬ ìµœì¢… ì •í™•ì„± ë³´ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸ“ ìŠ¤ë ˆë“œ ë° ë¸”ë¡ ì¸ë±ìŠ¤ ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    block_id = block_idx.x

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸ  ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹ - ë¸”ë¡ ë‚´ ìŠ¤ë ˆë“œ ê°„ ê³ ì† í†µì‹ 
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡° í™œìš©:
    # - Shared Memory: ë¸”ë¡ ë‚´ TPB(128)ê°œ ìŠ¤ë ˆë“œê°€ ê³µìœ 
    # - ì§€ì—°ì‹œê°„: ~100 clock cycles (Global Memory ëŒ€ë¹„ 100x ë¹ ë¦„)
    # - ëŒ€ì—­í­: ~1.5TB/s (GPU ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
    #
    # ë©”ëª¨ë¦¬ ë±…í¬ ìµœì í™”:
    # - 32ê°œ ë±…í¬ë¡œ êµ¬ì„±ëœ shared memoryì—ì„œ ë±…í¬ ì¶©ëŒ íšŒí”¼
    # - ì—°ì†ì  ì ‘ê·¼ íŒ¨í„´ìœ¼ë¡œ ìµœëŒ€ ì²˜ë¦¬ëŸ‰ ë‹¬ì„±
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()  # ìµœëŒ“ê°’ ë¦¬ë•ì…˜ìš©
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()  # í•©ê³„ ë¦¬ë•ì…˜ìš©

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-1ë‹¨ê³„: ìŠ¤ë ˆë“œë³„ ì…ë ¥ ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸° ìµœëŒ“ê°’ ì„¤ì •
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ê²½ê³„ ì²˜ë¦¬ (Boundary Handling):
    # - ë§ˆì§€ë§‰ ë¸”ë¡ì˜ ê²½ìš° input_sizeë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ
    # - ì´ˆê³¼ ìŠ¤ë ˆë“œë“¤ì€ min_finite ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ìµœëŒ“ê°’ ê³„ì‚°ì— ì˜í–¥ ì—†ìŒ
    #
    # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ì´ˆê¸°í™”:
    # - min_finite[dtype]: í•´ë‹¹ íƒ€ì…ì˜ ìµœì†Œ ìœ í•œê°’ (-âˆì— ê°€ê¹Œì›€)
    # - ì‹¤ì œ ë°ì´í„°ë³´ë‹¤ í•­ìƒ ì‘ì€ ê°’ì´ë¯€ë¡œ max ì—°ì‚°ì—ì„œ ì•ˆì „
    var thread_max: Scalar[dtype] = min_finite[dtype]()

    if global_i < input_size:
        thread_max = rebind[Scalar[dtype]](input[global_i])

    shared_max[local_i] = thread_max
    barrier()  # ğŸš§ ëª¨ë“  ìŠ¤ë ˆë“œê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œí•  ë•Œê¹Œì§€ ë™ê¸°í™”

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-2ë‹¨ê³„: íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # Binary Tree Reduction (O(log n) ë³µì¡ë„)
    #
    # ë‹¨ê³„ë³„ ë¦¬ë•ì…˜ ì˜ˆì‹œ (TPB=8ì¼ ë•Œ):
    # ```
    # ì´ˆê¸°: [10, 1, 8, -1, 0, -2, 3, 5]
    #
    # stride=4: Thread 0~3 í™œì„±
    # Thread 0: max(10, 0) = 10    Thread 1: max(1, -2) = 1
    # Thread 2: max(8, 3) = 8      Thread 3: max(-1, 5) = 5
    # ê²°ê³¼: [10, 1, 8, 5, 0, -2, 3, 5]
    #
    # stride=2: Thread 0~1 í™œì„±
    # Thread 0: max(10, 8) = 10    Thread 1: max(1, 5) = 5
    # ê²°ê³¼: [10, 5, 8, 5, 0, -2, 3, 5]
    #
    # stride=1: Thread 0ë§Œ í™œì„±
    # Thread 0: max(10, 5) = 10
    # ìµœì¢…: [10, 5, 8, 5, 0, -2, 3, 5] â†’ block_max = 10
    # ```
    #
    # ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸:
    # - í™œì„± ìŠ¤ë ˆë“œ ìˆ˜ ì ì§„ì  ê°ì†Œ: 128 â†’ 64 â†’ 32 â†’ 16 â†’ 8 â†’ 4 â†’ 2 â†’ 1
    # - ë¹„í™œì„± ìŠ¤ë ˆë“œëŠ” ìë™ìœ¼ë¡œ warp ë ˆë²¨ì—ì„œ ìµœì í™”ë¨
    # - barrier() í˜¸ì¶œë¡œ ë°ì´í„° ë ˆì´ìŠ¤ ì¡°ê±´ ë°©ì§€
    stride = TPB // 2  # ì´ˆê¸° stride: 64 (TPB=128ì¸ ê²½ìš°)

    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        # í™œì„± ìŠ¤ë ˆë“œë§Œ ì´ì›ƒ ë°ì´í„°ì™€ ë¹„êµ
        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        # í™œì„± ìŠ¤ë ˆë“œë§Œ ìµœëŒ“ê°’ ì—…ë°ì´íŠ¸
        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2  # ë‹¤ìŒ ë¼ìš´ë“œ: í™œì„± ìŠ¤ë ˆë“œ ìˆ˜ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ

    block_max = shared_max[0]  # ë¸”ë¡ì˜ ìµœëŒ“ê°’ (Thread 0ì— ì €ì¥ë¨)

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-3ë‹¨ê³„: ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì§€ìˆ˜ ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # í•µì‹¬ ê³µì‹: exp(x_i - block_max)
    # - block_max ë³´ì •ìœ¼ë¡œ ì§€ìˆ˜ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    # - ê° ë¸”ë¡ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
    #
    # ìˆ˜í•™ì  ì •ë‹¹ì„± (Log-Sum-Exp Trick):
    # ```
    # ì›ë˜: exp(x_i) / Î£exp(x_j)  â† ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜
    # ì•ˆì „: exp(x_i - c) / Î£exp(x_j - c)  â† c = max(x)ë¡œ ì •ê·œí™”
    # ```
    #
    # ì˜ˆì‹œ (block_max = 10ì¸ ê²½ìš°):
    # - input = [10, 8, 12, 6] â†’ exp([0, -2, 2, -4]) = [1.0, 0.135, 7.389, 0.018]
    # - ëª¨ë“  ì§€ìˆ˜ê°€ ì•ˆì „í•œ ë²”ìœ„ ë‚´ì—ì„œ ê³„ì‚°ë¨
    var exp_val: Scalar[dtype] = 0.0
    if global_i < input_size:
        exp_val = rebind[Scalar[dtype]](exp(input[global_i] - block_max))

    shared_sum[local_i] = exp_val
    barrier()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-4ë‹¨ê³„: íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ì§€ìˆ˜ í•©ê³„ êµ¬í•˜ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ëª©ì : local_sum = Î£ exp(x_i - block_max) ê³„ì‚°
    # - ìµœëŒ“ê°’ ë¦¬ë•ì…˜ê³¼ ë™ì¼í•œ íŠ¸ë¦¬ êµ¬ì¡° ì‚¬ìš©
    # - í•©ê³„ ì—°ì‚°ì´ë¯€ë¡œ ë§ì…ˆ(+) ì‚¬ìš© (ìµœëŒ“ê°’ì€ max í•¨ìˆ˜)
    #
    # ì„±ëŠ¥ íŠ¹ì„±:
    # - ì‹œê°„ ë³µì¡ë„: O(log TPB) = O(log 128) = 7ë‹¨ê³„
    # - ê³µê°„ ë³µì¡ë„: O(TPB) shared memory ì‚¬ìš©
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        # í™œì„± ìŠ¤ë ˆë“œë§Œ ì´ì›ƒ ê°’ê³¼ í•©ì‚°
        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        # í™œì„± ìŠ¤ë ˆë“œë§Œ í•©ê³„ ì—…ë°ì´íŠ¸
        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    block_sum = shared_sum[0]  # ë¸”ë¡ì˜ ì§€ìˆ˜ í•©ê³„ (Thread 0ì— ì €ì¥ë¨)

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-5ë‹¨ê³„: ë¸”ë¡ë³„ ê²°ê³¼ë¥¼ ì¤‘ê°„ ë²„í¼ì— ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ë©”ëª¨ë¦¬ êµ¬ì¡°:
    # - block_maxes[GRID_SIZE]: ê° ë¸”ë¡ì˜ local_max ì €ì¥
    # - block_sums[GRID_SIZE]: ê° ë¸”ë¡ì˜ local_sum ì €ì¥
    #
    # ì˜ˆì‹œ (GRID_SIZE=4):
    # - block_maxes = [max_0, max_1, max_2, max_3]
    # - block_sums = [sum_0, sum_1, sum_2, sum_3]
    #
    # ìµœì í™”: Thread 0ë§Œ ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ì— ì“°ê¸° (ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì ˆì•½)
    if local_i == 0:  # ë¸”ë¡ì˜ ëŒ€í‘œ ìŠ¤ë ˆë“œë§Œ ì €ì¥
        block_maxes[block_id] = rebind[Scalar[dtype]](block_max)
        block_sums[block_id] = rebind[Scalar[dtype]](block_sum)


fn reduce_interim_kernel[
    dtype: DType = DType.float32,
](
    final_vals: UnsafePointer[Scalar[dtype]],  # [global_max, global_sum]
    block_maxes: UnsafePointer[Scalar[dtype]],
    block_sums: UnsafePointer[Scalar[dtype]],
    grid_size: Int,
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2ë‹¨ê³„: Global Aggregation Kernel
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # ëª©ì : ëª¨ë“  ë¸”ë¡ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ ìµœì¢… ê¸€ë¡œë²Œ í†µê³„ë¡œ ì·¨í•©
    #
    # í•µì‹¬ ë„ì „ê³¼ì œ: ìˆ˜ì¹˜ì  ì•ˆì •ì„± ìœ ì§€í•˜ë©´ì„œ ë¸”ë¡ ê°„ ê²°ê³¼ í•©ì„±
    # - ê° ë¸”ë¡ì´ ì„œë¡œ ë‹¤ë¥¸ local_maxë¥¼ ê°€ì§€ë¯€ë¡œ ë‹¨ìˆœ í•©ì‚° ë¶ˆê°€
    # - ê¸€ë¡œë²Œ ê¸°ì¤€ì (global_max)ìœ¼ë¡œ ëª¨ë“  ê²°ê³¼ë¥¼ ì¬ì¡°ì • í•„ìš”
    #
    # ì…ë ¥ ë°ì´í„° êµ¬ì¡°:
    # - block_maxes[GRID_SIZE]: [local_max_0, local_max_1, local_max_2, local_max_3]
    # - block_sums[GRID_SIZE]: [local_sum_0, local_sum_1, local_sum_2, local_sum_3]
    #
    # ì¶œë ¥ ë°ì´í„° êµ¬ì¡°:
    # - final_vals[0]: global_max = max(block_maxes)
    # - final_vals[1]: global_sum = Î£(local_sum_i Ã— exp(local_max_i - global_max))
    #
    # ìˆ˜í•™ì  ì •ë‹¹ì„±:
    # ```
    # ê° ë¸”ë¡ì—ì„œ: local_sum_i = Î£ exp(x_j - local_max_i)
    # ê¸€ë¡œë²Œì—ì„œ: global_sum = Î£_i Î£_j exp(x_j - global_max)
    #           = Î£_i (local_sum_i Ã— exp(local_max_i - global_max))
    # ```
    #
    # ì„±ëŠ¥ íŠ¹ì„±:
    # - ë‹¨ì¼ ë¸”ë¡ ì‹¤í–‰ (grid_dim=(1,1))ìœ¼ë¡œ ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
    # - ì…ë ¥ í¬ê¸°: GRID_SIZE (ë³´í†µ 4~32ê°œ) - ë§¤ìš° ì‘ìŒ
    # - ì²˜ë¦¬ ì‹œê°„: ì „ì²´ ëŒ€ë¹„ 1% ë¯¸ë§Œ (ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ìŠ¤ë ˆë“œ ì„¤ì • ë° ë©”ëª¨ë¦¬ í• ë‹¹
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    local_i = thread_idx.x

    # ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹ (1ë‹¨ê³„ì™€ ë™ì¼í•œ íŒ¨í„´)
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-1ë‹¨ê³„: ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ëª©ì : global_max = max(local_max_0, local_max_1, ..., local_max_n)
    #
    # ì˜ˆì‹œ (GRID_SIZE=4):
    # - block_maxes = [10.5, 8.2, 12.1, 9.3]
    # - íŠ¸ë¦¬ ë¦¬ë•ì…˜: 10.5 vs 8.2 â†’ 10.5, 12.1 vs 9.3 â†’ 12.1
    # - ìµœì¢…: 10.5 vs 12.1 â†’ global_max = 12.1
    var thread_max: Scalar[dtype] = min_finite[dtype]()
    if local_i < grid_size:
        thread_max = block_maxes[local_i]

    shared_max[local_i] = thread_max
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸° (1ë‹¨ê³„ì™€ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜)
    stride = TPB // 2
    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2

    global_max = shared_max[0]  # ì „ì²´ ë°ì´í„°ì˜ ìµœëŒ“ê°’

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-2ë‹¨ê³„: ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ê¸€ë¡œë²Œ í•©ê³„ ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # í•µì‹¬ ê³µì‹: global_sum = Î£ (local_sum_i Ã— exp(local_max_i - global_max))
    #
    # ìˆ˜í•™ì  ë°°ê²½:
    # ```
    # ì›ë˜ ëª©í‘œ: Î£_i Î£_j exp(x_j - global_max)
    # ë³€í˜•: Î£_i [ Î£_j exp(x_j - local_max_i) Ã— exp(local_max_i - global_max) ]
    #     = Î£_i [ local_sum_i Ã— exp(local_max_i - global_max) ]
    # ```
    #
    # êµ¬ì²´ì  ì˜ˆì‹œ:
    # - Block 0: local_max=10.5, local_sum=25.3
    #   adjusted_sum_0 = 25.3 Ã— exp(10.5 - 12.1) = 25.3 Ã— 0.202 = 5.11
    # - Block 1: local_max=8.2, local_sum=18.7
    #   adjusted_sum_1 = 18.7 Ã— exp(8.2 - 12.1) = 18.7 Ã— 0.021 = 0.39
    # - Block 2: local_max=12.1, local_sum=42.8
    #   adjusted_sum_2 = 42.8 Ã— exp(12.1 - 12.1) = 42.8 Ã— 1.0 = 42.8
    # - Block 3: local_max=9.3, local_sum=31.2
    #   adjusted_sum_3 = 31.2 Ã— exp(9.3 - 12.1) = 31.2 Ã— 0.065 = 2.03
    #
    # ê²°ê³¼: global_sum = 5.11 + 0.39 + 42.8 + 2.03 = 50.33
    var adjusted_sum: Scalar[dtype] = 0.0
    if local_i < grid_size:
        adjusted_sum = block_sums[local_i] * rebind[Scalar[dtype]](
            exp(block_maxes[local_i] - global_max)
        )

    shared_sum[local_i] = adjusted_sum
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ê¸€ë¡œë²Œ í•©ê³„ êµ¬í•˜ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    global_sum = shared_sum[0]  # ì „ì²´ ë°ì´í„°ì˜ ì§€ìˆ˜ í•©ê³„

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-3ë‹¨ê³„: ìµœì¢… ê¸€ë¡œë²Œ í†µê³„ ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ì¶œë ¥ í˜•ì‹: final_vals[2] = [global_max, global_sum]
    # - 3ë‹¨ê³„ normalize_kernelì—ì„œ ì‚¬ìš©ë  í•µì‹¬ íŒŒë¼ë¯¸í„°
    # - ëª¨ë“  ì…ë ¥ ë°ì´í„°ì˜ ì •ê·œí™” ê¸°ì¤€ì  ì—­í• 
    if local_i == 0:  # ëŒ€í‘œ ìŠ¤ë ˆë“œë§Œ ìµœì¢… ê²°ê³¼ ì €ì¥
        final_vals[0] = rebind[Scalar[dtype]](global_max)
        final_vals[1] = rebind[Scalar[dtype]](global_sum)


fn normalize_kernel[
    layout: Layout,
    input_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, layout],
    input: LayoutTensor[mut=False, dtype, layout],
    final_vals: UnsafePointer[Scalar[dtype]],  # [global_max, global_sum]
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3ë‹¨ê³„: Final Normalization Kernel
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # ëª©ì : ëª¨ë“  ì…ë ¥ì„ ìµœì¢… softmax í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
    #
    # í•µì‹¬ íŠ¹ì§•:
    # - ì™„ë²½í•œ ë³‘ë ¬ì„±: ê° ìŠ¤ë ˆë“œê°€ ë…ë¦½ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì›ì†Œ ì²˜ë¦¬
    # - ìˆ˜ì¹˜ì  ì•ˆì •ì„±: global_max ê¸°ì¤€ì  ì‚¬ìš©ìœ¼ë¡œ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    # - í™•ë¥  ë¶„í¬ ë³´ì¥: ê²°ê³¼ì˜ í•©ì´ ì •í™•íˆ 1.0ì´ ë˜ë„ë¡ ë³´ì¥
    #
    # ì„±ëŠ¥ íŠ¹ì„±:
    # - ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í•œê³„: ëŒ€ë¶€ë¶„ ë©”ëª¨ë¦¬ ì½ê¸°/ì“°ê¸° ì‹œê°„ì´ ì§€ë°°ì 
    # - ë†’ì€ ë³‘ë ¬ì„±: SIZEê°œ ìŠ¤ë ˆë“œê°€ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥
    # - ë‹¨ìˆœí•œ ì—°ì‚°: ì§€ìˆ˜ í•¨ìˆ˜ì™€ ë‚˜ëˆ—ì…ˆë§Œ í•„ìš”
    #
    # ì…ë ¥/ì¶œë ¥ íŒ¨í„´:
    # - ì…ë ¥: ì›ë³¸ ë°ì´í„° input[SIZE], ê¸€ë¡œë²Œ í†µê³„ final_vals[2]
    # - ì¶œë ¥: ì •ê·œí™”ëœ í™•ë¥  ë¶„í¬ output[SIZE]
    # - ë©”ëª¨ë¦¬ ì ‘ê·¼: coalesced íŒ¨í„´ìœ¼ë¡œ ìµœëŒ€ ëŒ€ì—­í­ í™œìš©
    global_i = block_dim.x * block_idx.x + thread_idx.x

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ìµœì¢… Softmax í™•ë¥  ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    #
    # ê³µì‹: softmax(x_i) = exp(x_i - global_max) / global_sum
    #
    # ìˆ˜í•™ì  ì •ë‹¹ì„±:
    # ```
    # í‘œì¤€ softmax: exp(x_i) / Î£ exp(x_j)
    # ì•ˆì „í•œ softmax: exp(x_i - c) / Î£ exp(x_j - c)  (c = global_max)
    #
    # ì¦ëª…: exp(x_i - c) / Î£ exp(x_j - c)
    #     = [exp(-c) Ã— exp(x_i)] / [exp(-c) Ã— Î£ exp(x_j)]
    #     = exp(x_i) / Î£ exp(x_j)  (exp(-c) ìƒì‡„)
    # ```
    #
    # êµ¬ì²´ì  ì˜ˆì‹œ:
    # - input = [10.5, 8.2, 12.1, 9.3], global_max = 12.1, global_sum = 50.33
    #
    # - Thread 0: exp(10.5 - 12.1) / 50.33 = exp(-1.6) / 50.33 = 0.202 / 50.33 = 0.004
    # - Thread 1: exp(8.2 - 12.1) / 50.33 = exp(-3.9) / 50.33 = 0.020 / 50.33 = 0.0004
    # - Thread 2: exp(12.1 - 12.1) / 50.33 = exp(0) / 50.33 = 1.0 / 50.33 = 0.020
    # - Thread 3: exp(9.3 - 12.1) / 50.33 = exp(-2.8) / 50.33 = 0.061 / 50.33 = 0.001
    #
    # ê²€ì¦: 0.004 + 0.0004 + 0.020 + 0.001 â‰ˆ 1.000 âœ“
    #
    # ê²½ê³„ ì²˜ë¦¬: global_i >= input_sizeì¸ ìŠ¤ë ˆë“œëŠ” ìë™ìœ¼ë¡œ ë¬´ì‹œë¨
    if global_i < input_size:
        global_max = final_vals[0]  # ì „ì²´ ë°ì´í„°ì˜ ìµœëŒ“ê°’
        global_sum = final_vals[1]  # ì „ì²´ ë°ì´í„°ì˜ ì§€ìˆ˜ í•©ê³„

        # ìµœì¢… softmax í™•ë¥  ê³„ì‚°
        # - exp(input[global_i] - global_max): ìˆ˜ì¹˜ì  ì•ˆì •ì„± ë³´ì¥
        # - / global_sum: í™•ë¥  ë¶„í¬ ì •ê·œí™” (í•© = 1.0)
        output[global_i] = (
            rebind[Scalar[dtype]](exp(input[global_i] - global_max))
            / global_sum
        )


# =============================================================================
# Column-wise Softmaxë¥¼ ìœ„í•œ ìƒˆë¡œìš´ GPU ì»¤ë„ë“¤
# =============================================================================


fn reduce_block_kernel_2d_colwise[
    layout: Layout,
    batch_size: Int,
    feature_size: Int,
    dtype: DType = DType.float32,
](
    block_maxes: UnsafePointer[
        Scalar[dtype]
    ],  # [feature_size * blocks_per_col]
    block_sums: UnsafePointer[Scalar[dtype]],  # [feature_size * blocks_per_col]
    input: LayoutTensor[dtype, layout, MutableAnyOrigin],
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Column-wise: 1ë‹¨ê³„ - ê° ì—´ë³„ ë¸”ë¡ ë‹¨ìœ„ ë¦¬ë•ì…˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬ ì•„ì´ë””ì–´: ê° ì—´ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜, ì—´ ë‚´ì—ì„œëŠ” Buffer Reduction ì ìš©
    #
    # ë©”ëª¨ë¦¬ êµ¬ì¡° (BATCH_SIZE=2, FEATURE_SIZE=256, TPB=128 ì˜ˆì‹œ):
    # - Col 0: input[0:2, 0] â†’ block_maxes[0], block_sums[0]
    # - Col 1: input[0:2, 1] â†’ block_maxes[1], block_sums[1]
    # - ...
    # - Col 255: input[0:2, 255] â†’ block_maxes[255], block_sums[255]
    #
    # BATCH_SIZE > TPBì¸ ê²½ìš°:
    # - ê° ì—´ì„ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
    # - blocks_per_col = (BATCH_SIZE + TPB - 1) // TPB

    local_i = thread_idx.x
    block_id = block_idx.x

    # Column-wise ì¸ë±ì‹±: ì–´ë–¤ ì—´(feature)ê³¼ ì–´ë–¤ ë¸”ë¡(batch chunk)ì¸ì§€ ê³„ì‚°
    blocks_per_col = (batch_size + TPB - 1) // TPB
    feature_idx = block_id // blocks_per_col  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ íŠ¹ì„± ì¸ë±ìŠ¤
    batch_block_idx = block_id % blocks_per_col  # í˜„ì¬ ì—´ ë‚´ ë¸”ë¡ ì¸ë±ìŠ¤

    # ê²½ê³„ ì²´í¬: ìœ íš¨í•œ íŠ¹ì„± ë²”ìœ„ ë‚´ì—ì„œë§Œ ì²˜ë¦¬
    if feature_idx >= feature_size:
        return

    # í˜„ì¬ ìŠ¤ë ˆë“œê°€ ë‹´ë‹¹í•  ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤ ê³„ì‚°
    global_batch_idx = batch_block_idx * TPB + local_i

    # ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-1ë‹¨ê³„: ê° ì—´ ë‚´ì—ì„œ ë¸”ë¡ë³„ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var thread_max: Scalar[dtype] = min_finite[dtype]()
    if global_batch_idx < batch_size:
        # Column-wise ì¸ë±ì‹±: input[global_batch_idx, feature_idx]
        thread_max = rebind[Scalar[dtype]](input[global_batch_idx, feature_idx])

    shared_max[local_i] = thread_max
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ìµœëŒ“ê°’ ì°¾ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2

    var block_max = shared_max[0]  # í˜„ì¬ ë¸”ë¡ì˜ ìµœëŒ“ê°’

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-2ë‹¨ê³„: ì§€ìˆ˜ ê³„ì‚° ë° ë¸”ë¡ë³„ í•©ê³„ êµ¬í•˜ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var exp_val: Scalar[dtype] = 0.0
    if global_batch_idx < batch_size:
        exp_val = rebind[Scalar[dtype]](
            exp(input[global_batch_idx, feature_idx] - block_max)
        )

    shared_sum[local_i] = exp_val
    barrier()

    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ë¸”ë¡ ë‚´ ì§€ìˆ˜ í•©ê³„ êµ¬í•˜ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    var block_sum = shared_sum[0]  # í˜„ì¬ ë¸”ë¡ì˜ ì§€ìˆ˜ í•©ê³„

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1-3ë‹¨ê³„: ë¸”ë¡ë³„ ê²°ê³¼ë¥¼ ì¤‘ê°„ ë²„í¼ì— ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if local_i == 0:  # ë¸”ë¡ì˜ ëŒ€í‘œ ìŠ¤ë ˆë“œë§Œ ì €ì¥
        block_maxes[block_id] = rebind[Scalar[dtype]](block_max)
        block_sums[block_id] = rebind[Scalar[dtype]](block_sum)


fn reduce_interim_kernel_2d_colwise[
    dtype: DType = DType.float32,
](
    final_vals: UnsafePointer[
        Scalar[dtype]
    ],  # [feature_size * 2] (ê° ì—´ì˜ [global_max, global_sum])
    block_maxes: UnsafePointer[Scalar[dtype]],
    block_sums: UnsafePointer[Scalar[dtype]],
    feature_size: Int,
    blocks_per_col: Int,
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Column-wise: 2ë‹¨ê³„ - ê° ì—´ë³„ ê¸€ë¡œë²Œ í†µê³„ ê³„ì‚°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬: ê° ì—´ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ê¸€ë¡œë²Œ maxì™€ sum ê³„ì‚°
    # - final_vals êµ¬ì¡°: [col0_max, col0_sum, col1_max, col1_sum, ...]

    local_i = thread_idx.x
    feature_idx = block_idx.x  # ê° ë¸”ë¡ì´ í•˜ë‚˜ì˜ íŠ¹ì„±(ì—´)ì„ ë‹´ë‹¹

    # ê²½ê³„ ì²´í¬
    if feature_idx >= feature_size:
        return

    # ê³µìœ  ë©”ëª¨ë¦¬ í• ë‹¹
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-1ë‹¨ê³„: í˜„ì¬ ì—´ì˜ ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var thread_max: Scalar[dtype] = min_finite[dtype]()
    if local_i < blocks_per_col:
        # í˜„ì¬ ì—´ì˜ block_idx ê³„ì‚°
        col_block_idx = feature_idx * blocks_per_col + local_i
        thread_max = block_maxes[col_block_idx]

    shared_max[local_i] = thread_max
    barrier()

    # íŠ¸ë¦¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ì—´ë³„ ê¸€ë¡œë²Œ ìµœëŒ“ê°’ ì°¾ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()
        stride = stride // 2

    var global_max = shared_max[0]  # í˜„ì¬ ì—´ì˜ ê¸€ë¡œë²Œ ìµœëŒ“ê°’

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-2ë‹¨ê³„: í˜„ì¬ ì—´ì˜ ê¸€ë¡œë²Œ í•©ê³„ ê³„ì‚°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    var adjusted_sum: Scalar[dtype] = 0.0
    if local_i < blocks_per_col:
        col_block_idx = feature_idx * blocks_per_col + local_i
        adjusted_sum = block_sums[col_block_idx] * rebind[Scalar[dtype]](
            exp(block_maxes[col_block_idx] - global_max)
        )

    shared_sum[local_i] = adjusted_sum
    barrier()

    # íŠ¸ë¦¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ì—´ë³„ ê¸€ë¡œë²Œ í•©ê³„ êµ¬í•˜ê¸°
    stride = TPB // 2
    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()
        stride = stride // 2

    var global_sum = shared_sum[0]  # í˜„ì¬ ì—´ì˜ ê¸€ë¡œë²Œ í•©ê³„

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2-3ë‹¨ê³„: ì—´ë³„ ê²°ê³¼ë¥¼ ìµœì¢… ë²„í¼ì— ì €ì¥
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if local_i == 0:
        final_vals[feature_idx * 2] = rebind[Scalar[dtype]](
            global_max
        )  # [feature_idx][0]
        final_vals[feature_idx * 2 + 1] = rebind[Scalar[dtype]](
            global_sum
        )  # [feature_idx][1]


fn normalize_kernel_2d_colwise[
    layout: Layout,
    batch_size: Int,
    feature_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[dtype, layout, MutableAnyOrigin],
    input: LayoutTensor[dtype, layout, MutableAnyOrigin],
    final_vals: UnsafePointer[Scalar[dtype]],  # [feature_size * 2]
):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Column-wise: 3ë‹¨ê³„ - ê° ì—´ë³„ ìµœì¢… ì •ê·œí™”
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #
    # í•µì‹¬: ê° ì—´ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ìµœì¢… softmax ì •ê·œí™” ìˆ˜í–‰
    # output[batch_idx, feature_idx] = exp(input[batch_idx, feature_idx] - global_max) / global_sum

    local_i = thread_idx.x
    block_id = block_idx.x

    # Column-wise ì¸ë±ì‹±: ì–´ë–¤ ì—´(feature)ê³¼ ì–´ë–¤ ë¸”ë¡(batch chunk)ì¸ì§€ ê³„ì‚°
    blocks_per_col = (batch_size + TPB - 1) // TPB
    feature_idx = block_id // blocks_per_col
    batch_block_idx = block_id % blocks_per_col

    # ê²½ê³„ ì²´í¬
    if feature_idx >= feature_size:
        return

    # í˜„ì¬ ìŠ¤ë ˆë“œê°€ ë‹´ë‹¹í•  ì „ì—­ ë°°ì¹˜ ì¸ë±ìŠ¤
    global_batch_idx = batch_block_idx * TPB + local_i

    # ê²½ê³„ ì²´í¬: ìœ íš¨í•œ ë°°ì¹˜ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì²˜ë¦¬
    if global_batch_idx >= batch_size:
        return

    # í˜„ì¬ ì—´ì˜ ê¸€ë¡œë²Œ í†µê³„ ê°€ì ¸ì˜¤ê¸°
    var global_max = final_vals[feature_idx * 2]  # í˜„ì¬ ì—´ì˜ global_max
    var global_sum = final_vals[feature_idx * 2 + 1]  # í˜„ì¬ ì—´ì˜ global_sum

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 3ë‹¨ê³„: ìµœì¢… softmax ì •ê·œí™”
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ê³µì‹: output[i,j] = exp(input[i,j] - global_max_j) / global_sum_j
    # - ê° ì—´(j)ë³„ë¡œ ë…ë¦½ì ì¸ global_max_j, global_sum_j ì‚¬ìš©
    # - ìŠ¤íŠ¸ë¼ì´ë“œ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ (ìºì‹œ íš¨ìœ¨ì„± ë‹¤ì†Œ ë–¨ì–´ì§)
    output[global_batch_idx, feature_idx] = rebind[Scalar[dtype]](
        exp(input[global_batch_idx, feature_idx] - global_max) / global_sum
    )


fn softmax_gpu_kernel[
    layout: Layout,
    input_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, layout],
    input: LayoutTensor[mut=False, dtype, layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    # =========================================================================
    # 1ë‹¨ê³„: ë³‘ë ¬ ë¦¬ë•ì…˜ìœ¼ë¡œ ìµœëŒ“ê°’ ì°¾ê¸°
    # =========================================================================
    #
    # ê³µìœ  ë©”ëª¨ë¦¬ (Shared Memory) ì‚¬ìš© ì´ìœ :
    # - ëª¨ë“  ìŠ¤ë ˆë“œê°€ í˜‘ë ¥í•˜ì—¬ ìµœëŒ“ê°’ê³¼ í•©ê³„ë¥¼ ê³„ì‚°í•´ì•¼ í•¨
    # - ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ë³´ë‹¤ 100ë°° ì´ìƒ ë¹ ë¥¸ ì ‘ê·¼ ì†ë„
    # - GPU ë¸”ë¡ ë‚´ ìŠ¤ë ˆë“œë“¤ ê°„ì˜ íš¨ìœ¨ì ì¸ ë°ì´í„° ê³µìœ 
    #
    # ì˜ˆì‹œ: TPB=8ì¼ ë•Œ, 8ê°œ ìŠ¤ë ˆë“œê°€ ê°ê° í•˜ë‚˜ì˜ ì…ë ¥ê°’ì„ ë‹´ë‹¹
    # Thread 0: input[0], Thread 1: input[1], ..., Thread 7: input[7]
    shared_max = tb[dtype]().row_major[TPB]().shared().alloc()
    shared_sum = tb[dtype]().row_major[TPB]().shared().alloc()

    # ê° ìŠ¤ë ˆë“œê°€ ìì‹ ì´ ë‹´ë‹¹í•˜ëŠ” ì…ë ¥ê°’ì„ ê³µìœ  ë©”ëª¨ë¦¬ì— ì €ì¥
    var thread_max: Scalar[dtype] = min_finite[dtype]()

    if global_i < input_size:
        thread_max = rebind[Scalar[dtype]](input[global_i])

    shared_max[local_i] = thread_max

    barrier()  # ëª¨ë“  ìŠ¤ë ˆë“œê°€ ê°’ì„ ì €ì¥í•  ë•Œê¹Œì§€ ëŒ€ê¸°

    # =========================================================================
    # íŠ¸ë¦¬ ê¸°ë°˜ ë³‘ë ¬ ë¦¬ë•ì…˜ (Tree-based Parallel Reduction)
    # =========================================================================
    #
    # í† ë„ˆë¨¼íŠ¸ ë°©ì‹ìœ¼ë¡œ ìµœëŒ“ê°’ ì°¾ê¸° - O(log n) ì‹œê°„ ë³µì¡ë„
    #
    # êµ¬ì²´ì  ì˜ˆì‹œ (TPB=8, ì…ë ¥: [3, 1, 4, 1, 5, 9, 2, 6]):
    #
    # ì´ˆê¸°ìƒíƒœ: shared_max = [3, 1, 4, 1, 5, 9, 2, 6]
    #
    # 1ë¼ìš´ë“œ (stride=4):
    #   Thread 0: max(3, 5) = 5  â†’  shared_max[0] = 5
    #   Thread 1: max(1, 9) = 9  â†’  shared_max[1] = 9
    #   Thread 2: max(4, 2) = 4  â†’  shared_max[2] = 4
    #   Thread 3: max(1, 6) = 6  â†’  shared_max[3] = 6
    #   ê²°ê³¼: shared_max = [5, 9, 4, 6, 5, 9, 2, 6]
    #
    # 2ë¼ìš´ë“œ (stride=2):
    #   Thread 0: max(5, 4) = 5  â†’  shared_max[0] = 5
    #   Thread 1: max(9, 6) = 9  â†’  shared_max[1] = 9
    #   ê²°ê³¼: shared_max = [5, 9, 4, 6, 5, 9, 2, 6]
    #
    # 3ë¼ìš´ë“œ (stride=1):
    #   Thread 0: max(5, 9) = 9  â†’  shared_max[0] = 9
    #   ìµœì¢… ê²°ê³¼: ì „ì²´ ìµœëŒ“ê°’ = 9
    stride = TPB // 2

    while stride > 0:
        var local_max: Scalar[dtype] = min_finite[dtype]()

        if local_i < stride:
            local_max = rebind[Scalar[dtype]](shared_max[local_i + stride])

        barrier()

        if local_i < stride:
            shared_max[local_i] = max(shared_max[local_i], local_max)

        barrier()

        stride = stride // 2

    block_max = shared_max[0]  # ì „ì²´ ë¸”ë¡ì˜ ìµœëŒ“ê°’

    # =========================================================================
    # 2ë‹¨ê³„: ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì§€ìˆ˜ ê³„ì‚°
    # =========================================================================
    #
    # ìˆ˜í•™ì  ì •ë‹¹ì„±ê³¼ êµ¬ì²´ì  ì˜ˆì‹œ:
    # ì›ë˜ ê³µì‹: softmax(x_i) = exp(x_i) / Î£exp(x_j)
    # ì•ˆì „í•œ ê³µì‹: softmax(x_i) = exp(x_i - c) / Î£exp(x_j - c)  (c = max(x))
    #
    # ì˜ˆì‹œ: ì…ë ¥ [100, 101, 102], max = 102
    # - ì›ë˜: exp(100), exp(101), exp(102) â†’ ëª¨ë‘ ê±°ëŒ€í•œ ìˆ˜ (ì˜¤ë²„í”Œë¡œìš°)
    # - ì•ˆì „: exp(-2), exp(-1), exp(0) = [0.135, 0.368, 1.0] â†’ ì•ˆì „í•œ ë²”ìœ„
    #
    # ì¤‘ìš”: ìˆ˜í•™ì ìœ¼ë¡œ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ë©´ì„œ ì˜¤ë²„í”Œë¡œìš°ë§Œ ë°©ì§€
    var exp_val: Scalar[dtype] = 0.0

    if global_i < input_size:
        exp_val = rebind[Scalar[dtype]](exp(input[global_i] - block_max))
        output[global_i] = exp_val  # ì„ì‹œë¡œ ì§€ìˆ˜ê°’ ì €ì¥

    shared_sum[local_i] = exp_val

    barrier()

    # =========================================================================
    # 3ë‹¨ê³„: í•©ê³„ë¥¼ ìœ„í•œ ë‘ ë²ˆì§¸ ë³‘ë ¬ ë¦¬ë•ì…˜
    # =========================================================================
    #
    # Softmax íŠ¹ì„±ìƒ ë‘ ë‹¨ê³„ ë¦¬ë•ì…˜ì´ í•„ìš”í•œ ì´ìœ :
    # 1ë‹¨ê³„: ìµœëŒ“ê°’ êµ¬í•˜ê¸° (ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¨¼ì € í•„ìš”)
    # 2ë‹¨ê³„: ì§€ìˆ˜ì˜ í•© êµ¬í•˜ê¸° (ì •ê·œí™”ë¥¼ ìœ„í•´ ë‘ ë²ˆì§¸ë¡œ í•„ìš”)
    #
    # êµ¬ì²´ì  ì˜ˆì‹œ (ì•ì˜ ì§€ìˆ˜ê°’ë“¤ [0.135, 0.368, 1.0, ...]ì˜ í•© êµ¬í•˜ê¸°):
    #
    # 1ë¼ìš´ë“œ: ì¸ì ‘í•œ ê°’ë“¤ë¼ë¦¬ ë”í•˜ê¸°
    #   Thread 0: 0.135 + next_value
    #   Thread 1: 0.368 + next_value
    #   ...
    # ìµœì¢…: ëª¨ë“  ì§€ìˆ˜ê°’ì˜ í•©ê³„ (ì˜ˆ: 3.5)
    stride = TPB // 2

    while stride > 0:
        var local_sum: Scalar[dtype] = 0.0

        if local_i < stride:
            local_sum = rebind[Scalar[dtype]](shared_sum[local_i + stride])

        barrier()

        if local_i < stride:
            shared_sum[local_i] += local_sum

        barrier()

        stride = stride // 2

    block_sum = shared_sum[0]  # ëª¨ë“  ì§€ìˆ˜ê°’ì˜ í•©ê³„

    # =========================================================================
    # 4ë‹¨ê³„: ìµœì¢… í™•ë¥  ë¶„í¬ ìƒì„± (ì •ê·œí™”)
    # =========================================================================
    #
    # í™•ë¥  ë¶„í¬ë¡œì˜ ë³€í™˜:
    # - ì´ì „: ë‹¨ìˆœ ì§€ìˆ˜ê°’ [0.135, 0.368, 1.0] (í•©ê³„: 1.503)
    # - í˜„ì¬: í™•ë¥ ê°’ [0.090, 0.245, 0.665] (í•©ê³„: 1.0)
    #
    # ì •ê·œí™” ê³µì‹: softmax(x_i) = exp(x_i - max) / Î£exp(x_j - max)
    # ê²°ê³¼: ëª¨ë“  ê°’ì´ 0~1 ì‚¬ì´, ì „ì²´ í•©ì´ ì •í™•íˆ 1.0ì¸ í™•ë¥  ë¶„í¬
    if global_i < input_size:
        output[global_i] = output[global_i] / block_sum


# ANCHOR_END: softmax_gpu_kernel


# ANCHOR: softmax_cpu_kernel
fn softmax_cpu_kernel[
    layout: Layout,
    input_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[dtype, layout, MutableAnyOrigin],
    input: LayoutTensor[dtype, layout, MutableAnyOrigin],
):
    # =========================================================================
    # CPU vs GPU êµ¬í˜„ ë°©ì‹ì˜ í•µì‹¬ ì°¨ì´ì 
    # =========================================================================
    #
    # CPU ë²„ì „ íŠ¹ì§•:
    # - ìˆœì°¨ì  ì²˜ë¦¬: í•˜ë‚˜ì”© ì°¨ë¡€ëŒ€ë¡œ ê³„ì‚° (O(n) ì‹œê°„ ë³µì¡ë„)
    # - ë‹¨ìˆœí•œ for ë£¨í”„ ì‚¬ìš©
    # - ì§€ì—­ ë³€ìˆ˜ë¡œ ì„ì‹œ ê°’ ì €ì¥
    # - ì´í•´í•˜ê¸° ì‰¬ìš´ ì§ê´€ì  êµ¬ì¡°
    #
    # GPU ë²„ì „ íŠ¹ì§•:
    # - ë³‘ë ¬ ì²˜ë¦¬: ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ê³„ì‚° (O(log n) ì‹œê°„ ë³µì¡ë„)
    # - ë³µì¡í•œ ë³‘ë ¬ ë¦¬ë•ì…˜ ì•Œê³ ë¦¬ì¦˜
    # - ê³µìœ  ë©”ëª¨ë¦¬ë¡œ ìŠ¤ë ˆë“œ ê°„ í˜‘ë ¥
    # - ë†’ì€ ì„±ëŠ¥, ë³µì¡í•œ êµ¬ì¡°
    #
    # êµ¬ì²´ì  ì˜ˆì‹œ ë¹„êµ (ì…ë ¥: [2.0, 1.0, 0.1]):
    # CPU: ìŠ¤ë ˆë“œ 1ê°œê°€ ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ê³„ì‚° ìˆ˜í–‰
    # GPU: ìŠ¤ë ˆë“œ 3ê°œê°€ ë™ì‹œì— ê°ìì˜ ê°’ì„ ì²˜ë¦¬í•˜ë©° í˜‘ë ¥

    # =========================================================================
    # 1ë‹¨ê³„: ìµœëŒ“ê°’ ì°¾ê¸° (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    # =========================================================================
    #
    # ì˜ˆì‹œ: ì…ë ¥ [2.0, 1.0, 0.1]ì—ì„œ ìµœëŒ“ê°’ 2.0 ì°¾ê¸°
    # - ì´ˆê¸°ê°’: min_finite (ê°€ì¥ ì‘ì€ ê°€ëŠ¥í•œ ê°’)
    # - ìˆœì°¨ ë¹„êµ: min_finite â†’ 2.0 â†’ 2.0 â†’ 2.0 (ìµœì¢…)
    var max_val: Scalar[dtype] = min_finite[dtype]()

    for i in range(input_size):
        max_val = max(max_val, rebind[Scalar[dtype]](input[i]))

    # =========================================================================
    # 2ë‹¨ê³„: ì§€ìˆ˜ ê³„ì‚° ë° í•©ê³„ êµ¬í•˜ê¸°
    # =========================================================================
    #
    # ì˜ˆì‹œ ê³„ì‚° ê³¼ì • (ì…ë ¥: [2.0, 1.0, 0.1], max_val: 2.0):
    # i=0: exp(2.0 - 2.0) = exp(0.0) = 1.0,     sum_exp = 1.0
    # i=1: exp(1.0 - 2.0) = exp(-1.0) = 0.368,  sum_exp = 1.368
    # i=2: exp(0.1 - 2.0) = exp(-1.9) = 0.150,  sum_exp = 1.518
    #
    # ì„ì‹œ ê²°ê³¼: output = [1.0, 0.368, 0.150], sum_exp = 1.518
    var sum_exp: Scalar[dtype] = 0.0

    for i in range(input_size):
        var exp_val = rebind[Scalar[dtype]](exp(input[i] - max_val))
        output[i] = exp_val
        sum_exp += exp_val

    # =========================================================================
    # 3ë‹¨ê³„: ì •ê·œí™” (í™•ë¥  ë¶„í¬ ìƒì„±)
    # =========================================================================
    #
    # ìµœì¢… í™•ë¥  ê³„ì‚° (ì•ì˜ ì˜ˆì‹œ ê³„ì†):
    # i=0: 1.0 / 1.518 = 0.659     (65.9% í™•ë¥ )
    # i=1: 0.368 / 1.518 = 0.242   (24.2% í™•ë¥ )
    # i=2: 0.150 / 1.518 = 0.099   (9.9% í™•ë¥ )
    #
    # ê²€ì¦: 0.659 + 0.242 + 0.099 = 1.0 âœ“
    # ê²°ê³¼: ê°€ì¥ í° ì…ë ¥ê°’(2.0)ì´ ê°€ì¥ ë†’ì€ í™•ë¥ (65.9%)ì„ ê°€ì§
    for i in range(input_size):
        output[i] = output[i] / sum_exp


# ANCHOR_END: softmax_cpu_kernel

import compiler
from runtime.asyncrt import DeviceContextPtr
from tensor import InputTensor, OutputTensor


@compiler.register("softmax")
struct SoftmaxCustomOp:
    @staticmethod
    fn execute[
        target: StaticString,  # "cpu" or "gpu"
        mode: StaticString,  # "row_wise" or "col_wise"
        batch_size: Int,
        feature_size: Int,
        dtype: DType = DType.float32,
    ](
        output: OutputTensor[rank=2],
        input: InputTensor[rank = output.rank],
        ctx: DeviceContextPtr,
    ) raises:
        # Note: rebind is necessary now but it shouldn't be!
        var output_tensor = rebind[
            LayoutTensor[dtype, layout, MutableAnyOrigin]
        ](output.to_layout_tensor())
        var input_tensor = rebind[
            LayoutTensor[dtype, layout, MutableAnyOrigin]
        ](input.to_layout_tensor())

        alias layout = input_tensor.layout

        @parameter
        if target == "gpu":
            gpu_ctx = ctx.get_device_context()

            # ë³€ìˆ˜ë“¤ì„ ë¯¸ë¦¬ ì„ ì–¸ (ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°)
            var blocks_per_row: Int = 0
            var blocks_per_col: Int = 0
            var total_blocks: Int

            @parameter
            if mode == "row_wise":
                # Row-wise: 2D Batched Softmaxë¥¼ ìœ„í•œ ê·¸ë¦¬ë“œ ê³„ì‚°
                blocks_per_row = (feature_size + TPB - 1) // TPB
                total_blocks = batch_size * blocks_per_row
            elif mode == "col_wise":
                # Column-wise: 2D Batched Softmaxë¥¼ ìœ„í•œ ê·¸ë¦¬ë“œ ê³„ì‚°
                blocks_per_col = (batch_size + TPB - 1) // TPB
                total_blocks = feature_size * blocks_per_col
            else:
                raise Error("Unsupported mode: " + mode)

            # ë””ë°”ì´ìŠ¤ ë²„í¼ í• ë‹¹ (ê³µí†µ)
            var block_maxes_buffer = gpu_ctx.enqueue_create_buffer[dtype](
                total_blocks
            )
            var block_sums_buffer = gpu_ctx.enqueue_create_buffer[dtype](
                total_blocks
            )

            # Final values buffer í¬ê¸° ê³„ì‚°
            var final_vals_size: Int = 0

            @parameter
            if mode == "row_wise":
                final_vals_size = (
                    batch_size * 2
                )  # ê° í–‰ì˜ [global_max, global_sum]
            elif mode == "col_wise":
                final_vals_size = (
                    feature_size * 2
                )  # ê° ì—´ì˜ [global_max, global_sum]

            var final_vals_buffer = gpu_ctx.enqueue_create_buffer[dtype](
                final_vals_size
            )

            # 2D Batched Softmaxì—ì„œ GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”ì˜ ì¤‘ìš”ì„±
            # - ê° í–‰ë³„ë¡œ ë…ë¦½ì ì¸ softmax ê³„ì‚°
            # - ë°°ì¹˜ ì°¨ì›ë§Œí¼ ì¦ê°€í•œ ì¤‘ê°„ ë²„í¼ë“¤ì˜ ì •í™•í•œ ì´ˆê¸°í™” í•„ìˆ˜
            #
            # ì¶œë ¥ í…ì„œ ì´ˆê¸°í™”
            var total_elements = batch_size * feature_size
            gpu_ctx.enqueue_memset(
                DeviceBuffer[output_tensor.dtype](
                    gpu_ctx,
                    rebind[UnsafePointer[Scalar[output_tensor.dtype]]](
                        output_tensor.ptr
                    ),
                    total_elements,
                    owning=False,
                ),
                0,
            )

            # ì¤‘ê°„ ë²„í¼ë“¤ ì´ˆê¸°í™”
            # - block_maxes_buffer: ê° í–‰ë³„ ë¸”ë¡ ìµœëŒ“ê°’ ì €ì¥ìš©
            # - block_sums_buffer: ê° í–‰ë³„ ë¸”ë¡ í•©ê³„ ì €ì¥ìš© (ë°˜ë“œì‹œ 0ì´ì–´ì•¼ í•¨)
            # - final_vals_buffer: ê° í–‰ë³„ ìµœì¢… [global_max, global_sum] ì €ì¥ìš©
            gpu_ctx.enqueue_memset(block_maxes_buffer, 0.0)
            gpu_ctx.enqueue_memset(block_sums_buffer, 0.0)
            gpu_ctx.enqueue_memset(final_vals_buffer, 0.0)

            # 2D Batched Softmax 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (modeì— ë”°ë¼ ë¶„ê¸°)

            @parameter
            if mode == "row_wise":
                # Row-wise: ê° í–‰ë³„ ë…ë¦½ ì²˜ë¦¬

                # 1ë‹¨ê³„: ê° í–‰ë³„ ë¸”ë¡ ë‹¨ìœ„ ë¦¬ë•ì…˜ ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[
                    reduce_block_kernel_2d[
                        layout, batch_size, feature_size, dtype
                    ]
                ](
                    block_maxes_buffer.unsafe_ptr(),
                    block_sums_buffer.unsafe_ptr(),
                    input_tensor,
                    grid_dim=(total_blocks, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

                # 2ë‹¨ê³„: ê° í–‰ë³„ ê¸€ë¡œë²Œ í†µê³„ ê³„ì‚° ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[reduce_interim_kernel_2d[dtype]](
                    final_vals_buffer.unsafe_ptr(),
                    block_maxes_buffer.unsafe_ptr(),
                    block_sums_buffer.unsafe_ptr(),
                    batch_size,
                    blocks_per_row,
                    grid_dim=(batch_size, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

                # 3ë‹¨ê³„: ê° í–‰ë³„ ìµœì¢… ì •ê·œí™” ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[
                    normalize_kernel_2d[layout, batch_size, feature_size, dtype]
                ](
                    output_tensor,
                    input_tensor,
                    final_vals_buffer.unsafe_ptr(),
                    grid_dim=(total_blocks, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

            elif mode == "col_wise":
                # Column-wise: ê° ì—´ë³„ ë…ë¦½ ì²˜ë¦¬

                # 1ë‹¨ê³„: ê° ì—´ë³„ ë¸”ë¡ ë‹¨ìœ„ ë¦¬ë•ì…˜ ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[
                    reduce_block_kernel_2d_colwise[
                        layout, batch_size, feature_size, dtype
                    ]
                ](
                    block_maxes_buffer.unsafe_ptr(),
                    block_sums_buffer.unsafe_ptr(),
                    input_tensor,
                    grid_dim=(total_blocks, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

                # 2ë‹¨ê³„: ê° ì—´ë³„ ê¸€ë¡œë²Œ í†µê³„ ê³„ì‚° ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[
                    reduce_interim_kernel_2d_colwise[dtype]
                ](
                    final_vals_buffer.unsafe_ptr(),
                    block_maxes_buffer.unsafe_ptr(),
                    block_sums_buffer.unsafe_ptr(),
                    feature_size,
                    blocks_per_col,
                    grid_dim=(feature_size, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

                # 3ë‹¨ê³„: ê° ì—´ë³„ ìµœì¢… ì •ê·œí™” ì»¤ë„ ì‹¤í–‰
                gpu_ctx.enqueue_function[
                    normalize_kernel_2d_colwise[
                        layout, batch_size, feature_size, dtype
                    ]
                ](
                    output_tensor,
                    input_tensor,
                    final_vals_buffer.unsafe_ptr(),
                    grid_dim=(total_blocks, 1),
                    block_dim=THREADS_PER_BLOCK,
                )

        elif target == "cpu":

            @parameter
            if mode == "row_wise":
                # CPU Row-wise: ê° í–‰ë³„ softmax ì²˜ë¦¬
                for batch_idx in range(batch_size):
                    # í–‰ë³„ ìµœëŒ“ê°’ ì°¾ê¸°
                    var row_max: Scalar[dtype] = min_finite[dtype]()
                    for feature_idx in range(feature_size):
                        row_max = max(
                            row_max,
                            rebind[Scalar[dtype]](
                                input_tensor[batch_idx, feature_idx]
                            ),
                        )

                    # í–‰ë³„ ì§€ìˆ˜ í•©ê³„ ê³„ì‚°
                    var row_sum: Scalar[dtype] = 0.0
                    for feature_idx in range(feature_size):
                        var exp_val = rebind[Scalar[dtype]](
                            exp(input_tensor[batch_idx, feature_idx] - row_max)
                        )
                        output_tensor[batch_idx, feature_idx] = exp_val
                        row_sum += exp_val

                    # í–‰ë³„ ì •ê·œí™”
                    for feature_idx in range(feature_size):
                        output_tensor[batch_idx, feature_idx] = (
                            output_tensor[batch_idx, feature_idx] / row_sum
                        )

            elif mode == "col_wise":
                # CPU Column-wise: ê° ì—´ë³„ softmax ì²˜ë¦¬
                for feature_idx in range(feature_size):
                    # ì—´ë³„ ìµœëŒ“ê°’ ì°¾ê¸°
                    var col_max: Scalar[dtype] = min_finite[dtype]()
                    for batch_idx in range(batch_size):
                        col_max = max(
                            col_max,
                            rebind[Scalar[dtype]](
                                input_tensor[batch_idx, feature_idx]
                            ),
                        )

                    # ì—´ë³„ ì§€ìˆ˜ í•©ê³„ ê³„ì‚°
                    var col_sum: Scalar[dtype] = 0.0
                    for batch_idx in range(batch_size):
                        var exp_val = rebind[Scalar[dtype]](
                            exp(input_tensor[batch_idx, feature_idx] - col_max)
                        )
                        output_tensor[batch_idx, feature_idx] = exp_val
                        col_sum += exp_val

                    # ì—´ë³„ ì •ê·œí™”
                    for batch_idx in range(batch_size):
                        output_tensor[batch_idx, feature_idx] = (
                            output_tensor[batch_idx, feature_idx] / col_sum
                        )
        else:
            raise Error("Unsupported target: " + target)
